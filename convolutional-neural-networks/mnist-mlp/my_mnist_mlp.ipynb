{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP With Training, Validation and Test Sets\n",
    "\n",
    "In this notebook we build a simple MLP for the MNIST dataset. The interesting part is the split of the training `DataLoader` into a training and a validation part, by means of an instance of the `SubsetRandomSampler` class. In fact, it's not possible to directly index or subset a `DataLoader`. This is why the `sampler=` argument in a `DataLoader` is so important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written to run on my desktop computer. If you want to run it elsewhere you have to change the location where the MNIST dataset will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the MNIST images\n",
    "ROOT = '~/.pytorch'\n",
    "# number of subprocesses to use for data loading\n",
    "NUM_WORKERS = 0\n",
    "# how many samples per batch to load\n",
    "BATCH_SIZE = 32\n",
    "# percentage of training set to use as validation\n",
    "VALID_SIZE = 0.2\n",
    "# Size of the hidden layers\n",
    "HIDDEN_SIZE = 512\n",
    "# Size of the output layer\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "# Set device\n",
    "CUDA_DEVICE = 'cuda'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(CUDA_DEVICE)\n",
    "else:\n",
    "    torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: when we create the training and the validation `DataLoaders`, it appears to me that we are forced to use the same transformations for both. Therefore, how can I apply data augmentation to the training data loader but not to the validation one? Do we have to split the data in different folders? Probably.\n",
    "\n",
    "We start as usual from the training and test `DataLoaders` that come with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root=ROOT, train=True,\n",
    "                            download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=ROOT, train=False,\n",
    "                           download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the samplers for the training and the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(VALID_SIZE * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these samplers, we can build the three data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                                           sampler=train_sampler,\n",
    "                                           num_workers=NUM_WORKERS)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                                          num_workers=NUM_WORKERS)\n",
    "\n",
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, HIDDEN_SIZE)\n",
    "        self.fc2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.fc3 = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can define a function that runs the typical operations of a training or validation loop and, based on this, create a `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_loop(dataloader, model, criterion, optimizer, every=10, what='train'):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        batch_num += 1\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        if what == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        running_acc += (preds == labels).type(torch.IntTensor).sum()\n",
    "\n",
    "        if batch_num % every == 0:\n",
    "            print(('Batch N.: {:3}, ' + what.title() + 'Loss: {:.3f}, ' +\n",
    "                   what.title() + 'Acc.: {:.3f}').format(\n",
    "                       batch_num,\n",
    "                       running_loss.item()/(batch_num*dataloader.batch_size),\n",
    "                       running_acc.item()/(batch_num*dataloader.batch_size)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit function will run the `train_valid_loop` function with or without the computation of the gradients, based on the type of data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, criterion, optimizer, every):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'--- Epoch: {epoch} ---')\n",
    "        # Train loop\n",
    "        model.train()\n",
    "        print('\\n--- Train loop ----\\n')\n",
    "        train_valid_loop(train_loader, model, criterion,\n",
    "                         optimizer, every, 'train')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        print('\\n--- Validation loop ---\\n')\n",
    "        with torch.no_grad():\n",
    "            train_valid_loop(valid_loader, model, criterion,\n",
    "                             optimizer, every, what='valid')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the model, define the loss function and the optimizer, and run the training/validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- Epoch: 0 ---\n\n--- Train loop ----\n\nBatch N.: 100, TrainLoss: 12.142, TrainAcc.: 0.627\nBatch N.: 200, TrainLoss: 6.354, TrainAcc.: 0.735\nBatch N.: 300, TrainLoss: 4.396, TrainAcc.: 0.776\nBatch N.: 400, TrainLoss: 3.398, TrainAcc.: 0.805\nBatch N.: 500, TrainLoss: 2.794, TrainAcc.: 0.822\nBatch N.: 600, TrainLoss: 2.398, TrainAcc.: 0.834\nBatch N.: 700, TrainLoss: 2.107, TrainAcc.: 0.844\nBatch N.: 800, TrainLoss: 1.888, TrainAcc.: 0.851\nBatch N.: 900, TrainLoss: 1.711, TrainAcc.: 0.859\nBatch N.: 1000, TrainLoss: 1.575, TrainAcc.: 0.864\nBatch N.: 1100, TrainLoss: 1.461, TrainAcc.: 0.869\nBatch N.: 1200, TrainLoss: 1.370, TrainAcc.: 0.872\nBatch N.: 1300, TrainLoss: 1.289, TrainAcc.: 0.875\nBatch N.: 1400, TrainLoss: 1.218, TrainAcc.: 0.879\nBatch N.: 1500, TrainLoss: 1.160, TrainAcc.: 0.881\n\n--- Validation loop ---\n\nBatch N.: 100, ValidLoss: 0.252, ValidAcc.: 0.936\nBatch N.: 200, ValidLoss: 0.251, ValidAcc.: 0.935\nBatch N.: 300, ValidLoss: 0.266, ValidAcc.: 0.933\n--- Epoch: 1 ---\n\n--- Train loop ----\n\nBatch N.: 100, TrainLoss: 0.263, TrainAcc.: 0.939\nBatch N.: 200, TrainLoss: 0.268, TrainAcc.: 0.936\nBatch N.: 300, TrainLoss: 0.270, TrainAcc.: 0.936\nBatch N.: 400, TrainLoss: 0.284, TrainAcc.: 0.933\nBatch N.: 500, TrainLoss: 0.281, TrainAcc.: 0.932\nBatch N.: 600, TrainLoss: 0.284, TrainAcc.: 0.931\nBatch N.: 700, TrainLoss: 0.282, TrainAcc.: 0.931\nBatch N.: 800, TrainLoss: 0.284, TrainAcc.: 0.932\nBatch N.: 900, TrainLoss: 0.280, TrainAcc.: 0.933\nBatch N.: 1000, TrainLoss: 0.281, TrainAcc.: 0.933\nBatch N.: 1100, TrainLoss: 0.283, TrainAcc.: 0.932\nBatch N.: 1200, TrainLoss: 0.286, TrainAcc.: 0.932\nBatch N.: 1300, TrainLoss: 0.283, TrainAcc.: 0.933\nBatch N.: 1400, TrainLoss: 0.282, TrainAcc.: 0.932\nBatch N.: 1500, TrainLoss: 0.282, TrainAcc.: 0.932\n\n--- Validation loop ---\n\nBatch N.: 100, ValidLoss: 0.351, ValidAcc.: 0.923\nBatch N.: 200, ValidLoss: 0.319, ValidAcc.: 0.928\nBatch N.: 300, ValidLoss: 0.303, ValidAcc.: 0.929\n--- Epoch: 2 ---\n\n--- Train loop ----\n\nBatch N.: 100, TrainLoss: 0.236, TrainAcc.: 0.942\nBatch N.: 200, TrainLoss: 0.223, TrainAcc.: 0.943\nBatch N.: 300, TrainLoss: 0.232, TrainAcc.: 0.942\nBatch N.: 400, TrainLoss: 0.243, TrainAcc.: 0.941\nBatch N.: 500, TrainLoss: 0.238, TrainAcc.: 0.942\nBatch N.: 600, TrainLoss: 0.243, TrainAcc.: 0.941\nBatch N.: 700, TrainLoss: 0.243, TrainAcc.: 0.941\nBatch N.: 800, TrainLoss: 0.238, TrainAcc.: 0.942\nBatch N.: 900, TrainLoss: 0.237, TrainAcc.: 0.942\nBatch N.: 1000, TrainLoss: 0.243, TrainAcc.: 0.941\nBatch N.: 1100, TrainLoss: 0.243, TrainAcc.: 0.942\nBatch N.: 1200, TrainLoss: 0.241, TrainAcc.: 0.942\nBatch N.: 1300, TrainLoss: 0.244, TrainAcc.: 0.941\nBatch N.: 1400, TrainLoss: 0.242, TrainAcc.: 0.942\nBatch N.: 1500, TrainLoss: 0.242, TrainAcc.: 0.942\n\n--- Validation loop ---\n\nBatch N.: 100, ValidLoss: 0.324, ValidAcc.: 0.933\nBatch N.: 200, ValidLoss: 0.300, ValidAcc.: 0.938\nBatch N.: 300, ValidLoss: 0.311, ValidAcc.: 0.937\n"
    }
   ],
   "source": [
    "fit(3, model, criterion, optimizer, every=100)"
   ]
  }
 ]
}